#!/usr/bin/env python

import os
import sys
import requests
import json
import time
import pprint

DYNAMO_HOST = 't3btch057.mit.edu'
CERT = '/tmp/x509up_u%d' % os.getuid()

def prepare_request(request_id, cursor):
    now = int(time.time())

    sql = 'SELECT r.`RequestConfig`, r.`RequestVersion`, d.`DatasetProcess`, d.`DatasetSetup`, d.`DatasetTier`'
    sql += ' FROM `Requests` AS r'
    sql += ' INNER JOIN `Datasets` AS d ON d.`DatasetId` = r.`DatasetId`'
    sql += ' WHERE r.`RequestId` = %s'

    cursor.execute(sql, (request_id,))
    rows = cursor.fetchall()

    if len(rows) == 0:
        return {'dataset': [], 'datasetreplica': []}

    config, version, pd, sd, tier = rows[0]

    dataset_name = '%s/%s/%s+%s+%s' % (config, version, pd, sd, tier)

    data = {
        'dataset': [{'name': dataset_name, 'status': 'production', 'software_version': (config, version), 'last_update': now, 'is_open': True, 'blocks': []}],
        'datasetreplica': [{'dataset': dataset_name, 'site': 'T2_US_MIT', 'blockreplicas': []}]
    }

    blocks = data['dataset'][0]['blocks']
    blockreplicas = data['datasetreplica'][0]['blockreplicas']
        
    sql = 'SELECT b.`BlockName`, f.`FileName`, f.`SizeBytes` FROM `Blocks` AS b'
    sql += ' INNER JOIN `Requests` AS r ON r.`DatasetId` = b.`DatasetId`'
    sql += ' INNER JOIN `Lfns` AS l ON l.`BlockId` = b.`BlockId`'
    sql += ' LEFT JOIN `Files` AS f ON f.`FileName` = l.`FileName` AND f.`RequestId` = r.`RequestId`'
    sql += ' WHERE r.`RequestId` = %s'
    sql += ' ORDER BY b.`BlockName`'

    cursor.execute(sql, (request_id,))
    rows = cursor.fetchall()

    _block_name = ''
    for block_name, file_name, file_size in rows:
        if block_name != _block_name:
            _block_name = block_name

            blocks.append({'name': block_name,
                           'is_open': True,
                           'last_update': now,
                           'size': 0,
                           'num_files': 0,
                           'files': []})
            blockreplicas.append({'block': block_name, 'last_update': now, 'group': 'analysis'})
            block_data = blocks[-1]

        if file_name is not None:
            block_data['files'].append({'name': '/store/user/paus/%s/%s.root' % (dataset_name, file_name), 'size': file_size})
            block_data['size'] += file_size
            block_data['num_files'] += 1

    return data

if __name__ == '__main__':
    import MySQLdb
    from argparse import ArgumentParser
    
    # getting the commend line parameters
    parser = ArgumentParser(description = 'Generate Bambu database inventory.')
    parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', default = 'pandaf', help = 'Panda configuration.')
    parser.add_argument('--version', '-v', metavar = 'VERSION', dest = 'version', default = '010', help = 'Panda version.')
    parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'dataset', help = 'Panda dataset name.')
    args = parser.parse_args()
    sys.argv = []

    # make connection with database
    conn = MySQLdb.connect(read_default_file="/etc/my.cnf",read_default_group="mysql",db="Bambu")
    cursor = conn.cursor()

    # decode the dataset name
    pd, sd, tier = args.dataset.split('+')

    sql = 'SELECT r.`RequestId`'
    sql += ' FROM `Requests` AS r'
    sql += ' INNER JOIN `Datasets` AS d ON d.`DatasetId` = r.`DatasetId`'
    sql += ' WHERE r.`RequestConfig` = \'%s\' AND r.`RequestVersion` = \'%s\''%(args.config,args.version)
    sql += ' AND d.`DatasetProcess` = \'%s\' AND d.`DatasetSetup` = \'%s\' AND d.`DatasetTier` = \'%s\''%(pd,sd,tier)
    print ' SQL - %s'%sql
    cursor.execute(sql)
    rows = cursor.fetchall()
    if len(rows) != 1:
        sys.exit(0)

    # initialize
    data = {'dataset': [], 'datasetreplica': []}

    # get the request data
    req_data = prepare_request(rows[0][0], cursor)

    # add them to the empty data stub
    data['dataset'].extend(req_data['dataset'])
    data['datasetreplica'].extend(req_data['datasetreplica'])

    # write all of it to a file
    fileName = "%s_%s_%s.json"%(args.config,args.version,args.dataset)
    print " Filename: %s"%(fileName)
    with open(fileName,"w") as f:
        f.write(json.dumps(data))
